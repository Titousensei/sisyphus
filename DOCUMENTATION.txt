=== Introduction to Sisyphus ===

A Sisyphus program generally consists of one or several "pushes", which are
loops through all the rows of the inputs. The example above has 2 pushes. For
each push, the program will create a Pusher object, and declare the list of
actions to apply to each row (which can be modifications, lookups and outputs).
Finally, the program will tell the Pusher to push() the ouputs, which will start
the process of reading the inputs and processing the data.

Inputs of a push are processed as a single stream of rows. For each row, all
the actions will be executed (some actions can have an if condition). There's
no way to backtrack to a previous row.

Each action can have a separate schema, meaning that a given action will only
have a view of a subset of the global schema of the row. This allows some
outputs to only print the few columns they need in any order. It also has the
benefit of isolating the data for the code, and make sure a bug will not modify
the wrong columns.

Different actions can use the columns of the inputs or other actions simply by
using the same column names for their schema. Each row is loaded into the "row
container", which is a String array containing the value of all the column of
every schema of all the inputs and actions. The columns are not typed:
everything is a String. For best performance, the String value of each column
are populate "by reference": the objects are not copied, only the pointers.

As explain above, outputs are like any other action, so multiple outputs for
the same push are possible, including outputs with different columns and outputs
for different filtering conditions.

The example above also shows actions using of hashtables as in-memory lookup
tables. Sisyphus has several hashtables (in addition to the regular java
HashMaps), each for a different purpose and with different benefits. These
hashtables are designed to be persistent and can be saved on disk and loaded
from disk.

=== Schemas ===

Each input or action has an input schema and/or an output schema. A schema
consists of the list of names of columns.

Inputs and Outputs have only one schema. Because of the way actions link to each
other, Inputs have an output schema (the file is read and outputs some columns),
and Outputs have an input schema (the columns that come in to go out to a file).
Don't worry too much about that, just remember that they have one schema that
does what you would expect.

Modifiers have both an input and an output schema. For example, a hash function
requires the input of a few columns, and outputs a hash value.

Schema declaration often uses varargs. You can either use one String[] or
several String objects to declare your schema.

The pusher keeps the so-called "current row" with one value for each of the
columns declared by all the actions. However, each action will only see a subset
of the current row, called "view". The Pusher will create populate the view for
each action using the internal class SchemaAdapter.

Schemas of all the actions are validated before starting the push. The order of
the columns in different schemas might be the different, but all the input
columns of a given action must have been declare as an output column of at least
one previous step. The Pusher will throw a SchemaException at the start if
there is any mismatch.

If you're implementing custom Inputs, Outputs, or Modifier that need to remember
previous rows, please note that views are mutable and you will need to make a
copy (using System.arrayCopy if necessary)

Example:

  Inputs and actions declare their schemas:

      Input file declares its schema:
      - [id, lastname, firstname, company]

      A hashing Modifier declares its input and output schemas:
      - Input: [firstname, lastname];
      - Output: [hashvalue]

      Output file declares its schema:
      - [id, hashvalue]

  The overall schema for the "current row" will be:
  * [id, lastname, firstname, company, hashvalue]

  Now we start the Push. The current row contains has not yet been populated
  and might contain data from the previous rows that we must ignore.
  * [???, ???, ???, ???, ???]

      We read the next entry from the input file. The line might be:
      << "123\tGaudet\tEric\tTheFind\n"

      Input populates its view:
      - Output view: [123, Gaudet, Eric, TheFind]

  The values of the current row are now:
  * [123, Gaudet, Eric, TheFind, ???]

      The parameters passed to the Modifier will be two String[]:
      - Input view = [Eric, Gaudet]
      - Output view = [???]

      The modifier populates the output view
      - Output view = [ba1412839b468]

  The values of the current row are now:
  * [123, Gaudet, Eric, TheFind, ba1412839b468]

      The output file gets its view:
      - Input view: [123, ba1412839b468]

      We write the next line to the output file:
      >> "123\tba1412839b468\n"


=== Inputs ===

Inputs are the data that the Pusher iterates on. It populates the current row
with the entry from a single input at a time, until all the inputs are
processed.

The most common Inputs are text files, gzipped or not, in TSV format.

InputFile: single file, options to skip a header.
InputFileGroup: wildcard matching of the filenames, read each file in a serial
manner.

InputBinayFile: single file in binary format, with fixed-length records. Each
column can be any number of bytes. Values are presented as decimal or
hexadecimal.

Sisyphus can also iterate through the different types of hashtables:
InputKey, InputKeyMap, InputKeyBinding

Any iterable Object can get an associated Input, by extending the base class
Input (abstract).

There is also a few special Inputs used for sorting and joining:
InputMergeSorted, InputJoinSorted (parallel read, see below)

Custom inputs can be implemented easily by extending the class InputCustom. See
InputSplitRows in examples for a class that splits merged rows.

=== Outputs ===

Outputs are the data actually used for each row, either stored in files, or in
memory. File outputs will throw an execution exception by default if the file
already exists. This is to prevent the process to overwrite its own data by
mistake.

Outputs declares their schema, generally only a subset of the current row. The
columns can be in a different order.

OutputFile: write each row into a file.

Hashtables and HashMaps can be populated using Outputs: OutputKey, OutputKeyMap,
OutputKeyBinding, OutputHashMap

OutputFileSplit (abstract): write each row into one of the files of a group. The
most used implementation is SplitByOneColumn, where each row goes into the file
with the name corresponding to the content of one column. Re-using the example
above, one Output could split the DVD titles into one file per "category":
titles_action.tsv.gz, titles_comedy.tsv.gz, etc.

OutputSortSplit: used to sort files. (See "Sorting" below)

Outputs to files also creates a metadata file ".meta.<filename>", which
currently contains only the schema. This file is just for information at this
time, but future releases will use the data to automatically populate the schema
of inputs and enforce column sorting verifications for joins.

Custom outputs can be implemented easily by extending the class OutputCustom.
See OutputConcatRows in examples for a class that merges rows.

=== Keys ===

Keys are a powerful family of classes used by Sisyphus. They can be used as
indexes (like database indexes), look-up tables, counters, or set operations.

Keys are based on NativeHash family of classes: they store hashes, of the
native type long (8 bytes), with an optional value, which can be an int, a long
or a double. Since the data in Keys uses native types, they are extremely
memory-efficient. (For comparison, java HashSet can hold 10M Long values per
GB.)

- KeySet: just a set of hashes (long), without a lookup value. KeySets are
typically used for uniqueness detection, or to remember a list of hashes.
KeySets can hold 126M hashes per GB (8.1 bytes per entry in average).

- KeyMap: the most versatile Key, which keep hash->int-value relationships.
Since the value is an int, it can represent a count of things. That's why KeyMap
provides arithmetic operations for the values, such as increment. KeyMap is used
to keep counters, indices, 32-bytes hashes or ids. KeyMaps can hold 84M
hash->int-value pairs per GB (12.1 bytes per entry in average).

- KeyBinding: used to keep id-to-id bindings (mappings). KeyBinding keeps
hash-->hash relationships, and thus does not provide arithmetic operations for
its values. KeyBinding can hold 63M hash-hash pairs per GB (16.1 bytes per entry
in average).

- KeyDouble: used to arithmetic calculations in a double. Provides increment and
multiply of values. KeyDouble can hold 63M hash->double pairs per GB (16.1 bytes
per entry in average).

All these classes use the same base class Key, which provide the basic add,
remove, contains and get operations. They can be serialized to disk and loaded
from disk (uses java serialization, not compressed). Typically, 100M entries
KeyBinding is saved in 20s, and loaded in 40s. All Keys can be casted to a
KeySet using key.asKeySet(): this will create a new KeySet with just a pointer
to the hash part of a KeyMap, KeyBinding or KeyDouble (without allocating new
memory).

When serializing to disk, the filename recommendations are:
- _key.<keyname>,
- _map.<keyname>.<valuename>,
- _bind.<keyname>.<valuename>,
- _double..<keyname>.<valuename>
(This not currently enforced, but might be in the future.)

Alternatively, there is a utility method to load from a TSV file (slower than
serialization):
- Key.loadFromFile: load 1 or 2 columns from a TSV file

All Keys are iterable, and can be used as an Input using their corresponding
Input wrapper: InputKey, InputKeyMap, InputKeyBinding, InputKeyDouble. Their
schema is carried automatically.

Keys can also be populated using their corresponding Output wrappers: OutputKey,
OutputKeyMap, OutputKeyBinding, OutputKeyDouble.

Set operations can be done with the following utility methods:
- key.filterExclude(Key ref): remove from "key" all keys in "ref".
- key.filterOnly(Key ref): remove from "key" all keys not in "ref".
- key.filterExclude(Test tst): remove from "key" all entries
                                for which "tst" returns true.

KeyMap also has a utility function to do a "groupby+count" of the values of a
source KeyMap or a KeyBinding. CountValues increment the values where the hashes
are the value of the source. The column name of the destination hash must be the
same as source value.
- dst.countValues(KeyMap src)
- dst.countValues(KeyBinding src)

Finally, all the Key classes have a main method to query a serialized file.
Example:
- KeyMap <filepath> [<command>] [<list of hashes or values>]
where <command> can be one of:
- key <list of hashes> (default): lookup the entries with hashes in this list
- value <list of values>: find the entries with values in this list
- count <list of values>: "groupby+count" the entries with values in this list
- prompt: open a shell to lookup hashes

=== Modifiers ===

Users can create custom functions(a.k.a. "UDF") by extending the Modifier
(abstract) base class.

Typically, a Modifier will generate one output value from one or more input
values. The constructor parameters of most modifiers is in the form
(String out_value, String... in_values) because of the varargs. In some cases
(string split, for instance), in_value is first and out_values is last, for
the same reason.

When implementing a Modifier, you only need to implement the method:
- Declare schema in constructor: super(in_cols, out_cols);
- implement public void compute(String[] input, String[] result)

The length of String[] input will be exactly the same as the length of in_cols,
and the length of String[] result will be exactly the same as the length of
out_cols.

Here's a few useful Modifier provided by Sisyphus:
- KeyMapIncrement: increment the value of a KeyMap entry by a number
- KeyMapGetter: read a value of a KeyMap into a column
- KeyMapSetter: sets a value of a KeyMap to an int
- KeyDeleter: delete an entry in a KeySet/KeyMap/KeyBinding
- ColumnCopy, ColumnSwap: copy or swap the value of one column into another
- ColumnSet: sets the value of a column to an int, long or a String
- ColumnsHashLong: compute a single hash value for one or more columns combined.
- ColumnTrim: output column = input removed leading and trailing spaces
- ColumnToLower: output column = input to lower case
- ColumnRegex: match a regex to one input column, and write the groups to
               multiple output columns.

=== Pusher ===

This is the main processing class.

Declaration of the actions to be done for each row can be chained in a
script-like declarations. Finally, the push(input) command will start the
execution.

Contract:
- The order of the rows from the input is preserved in the outputs.
- The actions will be performed in the order of declaration for each row.

Pusher runs the actions sequentially in a single thread, because in most cases
the I/O is the bottleneck and the processing does not benefit from being multi-
threaded. If the CPU is the bottleneck (because of some custom modifier, for
instance), there is an option to run rows in parallel, with the command
parrallel(num_threads). However, running in parallel will not preserve the row
order between the input and the output. (The order of the actions is still
guaranteed.)

Typical speed (depending on the hardware):
- 2M rpm (rows per minute) for file inputs and file outputs
- 10M rpm input from files, outputs to memory
- 100M rpm for memory only processing

Pusher can be given a label to distinguish their log messages. It can also be
invoked with a limit (to process only the first N rows), or to process only
a random sample of the rows.

Example:

  String[] data_schema = new String[] { "guid", "category", "url", "title" };

  Input in_data = new InputFileGroup(DIR_INDEX, "newdata.*.gz", data_schema);

  KeyBinding bind_prod_hash = KeyMap.load(FILE_PROD_HASH); // [guid, #prod]
  Modifier prod_to_hash = new ColumnsHashLong("#prod", "category", "title");

  Output out_new      = new OutputFile(FILE_NEW,     data_schema);
  Output out_changed  = new OutputFile(FILE_CHANGED, data_schema);
  Output out_same     = new OutputFile(FILE_SAME,    "guid");

  new Pusher()
      .always(prod_to_hash) // populate #prod
      .ifMiss( bind_prod_hash, new BreakAfter(out_new))
      .ifDiff( bind_prod_hash, new BreakAfter(out_changed))
      .ifMatch(bind_prod_hash, new BreakAfter(out_same))
      .push(in_data);

=== Conditional Actions ===

Tests are a way to do conditional actions depending on the current row.

A number of built-in tests are provided by the Pusher:

1) no test (Action)
  - always: no test, the action will always occurs, regardless of the current row.

2) direct tests (Action):
  - ifNull: if column is null or empty, then execute action
  - ifNotNull: if column is not null and not empty, then execute action

3) Key-only tests: (KeySet/KeyMap/KeyBinding/KeyDouble, Action)
  - ifFound: if Key contains hash, then execute action
  - ifMiss: if Key does not contain hash, then execute action
  - ifDup: if Key already contains hash, execute Action; also store hash,
           and value if present. This is a combination of the frequent usage
           pattern to find duplicates: ifFound + always(OutputKey).

4) Key+value tests: (KeyMap/KeyBinding/KeyDouble, Action)
  - ifMatch: if Key contains hash, and value is the same, then execute action
  - ifDiff: if Key contains hash, and the value is different, then execute action

5) complex tests:
  - onlyIf(Test, Action): evaluate test condition and execute action if true.

Many complex tests are provided in com.thefind.sisyphus.test.*

They can compare a column with a value: IfLessThan, IfLessOrEquals,
IfGreaterThan, IfGreaterOrEquals

They can compare two or more columns: IfColumnsEqual, IfColumnsNotEqual

They can be combined: IfEquals, IfNotEquals IfNot, IfAnd, IfOr

Custom tests can be implemented easily, for example IsNumber.

(TODO: implement Key tests as complex test to be used in combinations)

=== Break Actions ===

Currently, Sisyphus does not support grouping and nesting of actions. Instead,
there is the BreakAfter action, which stops the chain of actions for this rows
and continue to the next row. (Similar to "continue" in a loop).

- new BreakAfter(Action): execute one last action, then break
- BreakAfter.NO_OP: just break, no last action. (constant provided for convenience.)
- new BreakAfter("label"): same as NO_OP, but prints an internal counter
                           with the label in the log
- new BreakAfter(Action, "label"): same as above.

=== Sorting ===

One important tool of data processing is sorting: once the data is sorted, it
becomes easy to do special processing that involves some kind of grouping or
joining.

Sisyphus provides the sorting functionality as an Output, in the form of
OutputSortSplit. The data is split into large sorted chunks, sorted in memory,
and saved into individual files with the same schema.

The user can define the memory allocation (typically 1-10M rows), and the
comparator to use: RowComparator (the defaul, for String comparisons) and
RowComparatorInteger are provided, but custom comparators can be easily
implemented.

The result is a group of files, each individually sorted. In most cases, these
files will be used immediately for further processing, such as joins. Sometimes,
we just want to merge the files and get one big sorted file. In both cases, we
will use InputMergeSorted, which will open all the splits at the same time, and
read the row in order (assuming the same comparator is used).

This one-and-a-half-step sorting technique allows Sisyphus to sort very large
files quite fast. Most of the time, the split files are temporary files and
can be discarded.

OutputSortSplit has a neat feature for data that is already partially sorted:
the biggest value of the previous chunk is remembered, and it will keep
appending to the same file as long as it can if the data is bigger than than the
biggest value of the previous chunk. This way, partially sorted data will have
less splits.

OutputSortSplit.sortOneFile is a utility method implementing the simplest and
most used case of sorting just one file on some columns:

  Input in_rows = new InputFile(input_file, file_schema);
  Output sort = new OutputSortSplit(output_file, batch_size, sort_schema, file_schema);
  new Pusher()
      .always(sort)
      .push(in_rows);

=== Joining ===

In the SQL world, even simple lookups are expressed as joins. In Sispyhus,
lookups are done using Keys or HashMaps. Only multi-columns joins need to be
expressed as joins.

Joins in Sisyphus can only be performed with inputs sorted on the same columns.
Data sets (files or keys) can be sorted using OutputSortSplit (see "Sorting"
above). Please note that InputMergeSorted can be used directly by the join and
it is not necessary to merge the split files into one. To summarize, joins need
2 steps: first step is to sort the data, second step is to join the data.

InputJoinSorted(String[] join_schema, Input... inputs) will load rows from each
input as necessary and performs an inner join, according to the join_schema,
into the current row. The columns in the join_schema will only appear once in
the current row. If different inputs have columns with the same name, only the
value from the first value (in order of the inputs declaration) will be used
for the current row. (This behavior might change in the future.)

Example: InputJoinSorted
  input1 -> [id, a1, b1]
  input2 -> [id, a2]
  input3 -> [a3, id]
  join_schema -> [id]
  [Pusher] ... row schema: [id, a1, b1, a2, a3]

InputLeftJoinSorted performs a left outer join. The first input is the "left"
input, and all its rows will be present. The remaining inputs are "right" inputs
and some their rows will be empty if they don't join.

InputSelfJoinSorted performs a self join of one input, while loading the data
only once. The schema of the current row will use the column names with the prefix
"r." for the "right" version of the input.

Example: InputSelfJoinSorted
  input1 -> [id, a1, b1]
  join_schema -> [id]
  [Pusher] ... row schema: [id, a1, b1, r.a1, r.b1]

=== Other Useful Tools ===

Pusher has a counter for every action and Key-related access. It will print
them in the log at the end of the push. Example:

  [Pusher] close: ActionIfKeyMap{[id, hash]?match -> OutputFile{[id, url, title,
  hash] -> "products_same.gz" +60,985,506 rows} 60,985,506 used, 25 warnings
  [Pusher] close: ActionIfKey{[urlhash]?found -> BreakAfter"Duplicate New Urls"
  2,390,849 used}

Pusher has powerful debugging options:
- debug(): print every current row after all the actions have been performed
- debug(n>0): print the first n rows
- debug(-1): print a row for each progress marker (every 1M)

Pusher also has a built-in profiler, to measure the time taken by each action
and print a summary at the end. This is mostly useful when several cpu-intensive
Modifiers are used. Please note that the processing will be slower when enabled
(about x2).

The class IOUtils provides utility methods for file management:
- assertSpace(), assertDir(), assertFile(): before you start the processing,
you can make sure there is enough space on disk, and that the proper files and
directories are present.
- rename: rename a file or directory; exception if destination exists.
- replace: rename a file or directory; delete or backup destination if exists.
- deleteDirectory: recursively delete a directory.

Finally, the class BackgroundShell allows a java program to run shell commands,
either directly or in a background thread. The stdout and stderr will can
captured and printed in the log when the shell finishes.

=== Common Use Case: Daily Process ===

Since the input files are read-only, the mechanism to update your data is not
immediately apparent. A very common use case is when a process updating the same
data set should occur daily.

The recommended solution is to have one directory called "input", containing the
current data set, and a second directory called "output", where the process is
going to write the updated data set. A third directory called "tmp" is also
often useful for all the temporary data. At the end of the process, when the
"output" directory is fully populated, the "tmp" directory can be deleted, the
"input" directory is either deleted or renamed "input.old" of archiving, and the
"output" directory is renamed "input", ready of the next day.

This setting is recommended for most Sisyphus programs, because it preserves and
separates the input data from the output data. If there was any problem during
the process, the "input" directory is still intact and the program can be easily
retried and debugged if necessary.

=== Example Implementation ===

Here's the code corresponding to the DVDs example from the README.

  // first push: read rented.txt into a hashtable
  KeyMap rented = new KeyMap("#title", "new_rented"); // long->int hashtable
  new Pusher()
      .always(new ColumnHashLong("#title", "title"))  // long=hash(string)
      .always(new KeyMapIncrement(rented, 1))
      .push(new InputFile("rented.txt", "title));

  // second push: do the same loop as before, with one more operation
  static final String[] SCHEMA = new String[] {
      "title", "category", "msrp", "price", "num_rented"
  };

  // second push: read the data and update the columns
  Input in_daily    = new InputFile("dvd.tsv.gz", SCHEMA);
  Modifier sale_mod = new SalePrice("price", "msrp", 0.9);
  Output out_daily  = new OutputFile("dvd_new.tsv.gz", SCHEMA);

  new Pusher()
      .onlyIf(new IfEquals("category", "comedy"), sale_mod)
      .always(new ColumnHashLong("#title", "title"))
      .ifMiss(rented, BreakAfter.NO_OP) // skip the rest if not in KeyMap rented
      .always(new KeyMapGetter(rented))) // populate new_rented
                  // "num_rented" = "num_rented" + "new_rented"
      .always(new Add("num_rented", "num_rented", "new_rented"))
      .push(in_daily);

  // implementation of the sales price computation
  public class SalePrice extends Modifier
  {
    protected final double price_coef_;

    public ColumnCopy(String outcol, String incol, double price_coef)
    {
      super(new String[] { incol }, new String[] { outcol });
      price_coef_ = price_coef;
    }

    public void compute(String[] input, String[] result)
    {
      try {
        double msrp = Double.parseDouble(input[0]);
        result[0] = String.format("%.2f", msrp * price_coef_);
      }
      catch (NumberFormatException nfex) {
        result[0] = null;
      }
    }
  }

